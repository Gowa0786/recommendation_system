{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e9def24-6a3d-40c4-bcdb-9d2246b17744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "\n",
    "engine = create_engine(\n",
    "    \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "    \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "df_likes = pd.read_sql(\n",
    "    \" SELECT timestamp, user_id, post_id FROM  public.feed_data WHERE  target = 1;\",con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e0aab8e-82b4-4969-90ab-4812cf286610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes = df_likes.sort_values('timestamp').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "325813c2-f5cb-4cf8-a375-fe3dccef2e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>22276</td>\n",
       "      <td>5717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>136194</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>122594</td>\n",
       "      <td>4554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>156604</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>52958</td>\n",
       "      <td>3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206340</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>32054</td>\n",
       "      <td>5367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206341</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>145967</td>\n",
       "      <td>5042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206342</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>125564</td>\n",
       "      <td>1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206343</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>152771</td>\n",
       "      <td>1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206344</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>90165</td>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8206345 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  user_id  post_id\n",
       "0       2021-10-01 06:01:40    22276     5717\n",
       "1       2021-10-01 06:01:40   136194     1205\n",
       "2       2021-10-01 06:01:40   122594     4554\n",
       "3       2021-10-01 06:01:40   156604      573\n",
       "4       2021-10-01 06:01:40    52958     3082\n",
       "...                     ...      ...      ...\n",
       "8206340 2021-12-29 23:43:27    32054     5367\n",
       "8206341 2021-12-29 23:43:27   145967     5042\n",
       "8206342 2021-12-29 23:43:27   125564     1720\n",
       "8206343 2021-12-29 23:43:27   152771     1202\n",
       "8206344 2021-12-29 23:43:27    90165      841\n",
       "\n",
       "[8206345 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "200f2548-1c91-41b8-8eca-bdf4e2e0bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes.to_csv('all_likes_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09e4f973-b0d1-4c86-bfc1-402157f1c43a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Å–µ–≥–æ –ª–∞–π–∫–æ–≤: 8,206,345\n",
      "–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: 6,831\n",
      "–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: 163,202\n",
      "\n",
      "Split –≤—Ä–µ–º—è: 2021-12-12 00:59:05.600000\n",
      "Train –ø–µ—Ä–∏–æ–¥: 2021-10-01 06:01:40 - 2021-12-12 00:59:05.600000\n",
      "Test –ø–µ—Ä–∏–æ–¥: 2021-12-12 00:59:05.600000 - 2021-12-29 23:43:27\n",
      "\n",
      "Train –ª–∞–π–∫–æ–≤: 6,326,477\n",
      "Test –ª–∞–π–∫–æ–≤: 1,879,868\n",
      "Test –ª–∞–π–∫–æ–≤ –ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è cold-start: 1,879,751\n",
      "\n",
      "–†–∞–∑–º–µ—Ä train –º–∞—Ç—Ä–∏—Ü—ã: (163168, 6831)\n",
      "–ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å: 0.5638%\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:31<00:00,  6.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –ë–ï–ó –£–¢–ï–ß–ï–ö!\n",
      "   User factors: (163168, 20) (—Ç–æ–ª—å–∫–æ –¥–ª—è 163,168 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ train)\n",
      "   Post factors: (6831, 20) (—Ç–æ–ª—å–∫–æ –¥–ª—è 6,831 –ø–æ—Å—Ç–æ–≤ –∏–∑ train)\n",
      "   Train –ª–∞–π–∫–æ–≤: 6,326,477\n",
      "   Test –ª–∞–π–∫–æ–≤: 1,879,751 (—Ç–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏ –ø–æ—Å—Ç—ã –∏–∑ train)\n",
      "\n",
      "üìå –¢–µ–ø–µ—Ä—å –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LGBM:\n",
      "   - –î–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ train –ø–µ—Ä–∏–æ–¥–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–∏ —Ñ–∞–∫—Ç–æ—Ä—ã\n",
      "   - –î–ª—è –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–ø–æ—Å–ª–µ split_time): —Ñ–∞–∫—Ç–æ—Ä—ã = 0 –∏–ª–∏ NaN\n",
      "   - –≠—Ç–æ—Ç –∂–µ split_time –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è LGBM –¥–∞—Ç–∞—Å–µ—Ç–∞!\n"
     ]
    }
   ],
   "source": [
    "# –°–ö–†–ò–ü–¢ –°–û–ó–î–ê–ù–ò–Ø –ê–õ–° –§–ê–ö–¢–û–†–û–í\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import pickle\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "df_likes = pd.read_csv('all_likes_df.csv')\n",
    "df_likes['timestamp'] = pd.to_datetime(df_likes['timestamp'])\n",
    "\n",
    "print(f\"–í—Å–µ–≥–æ –ª–∞–π–∫–æ–≤: {len(df_likes):,}\")\n",
    "print(f\"–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {df_likes['post_id'].nunique():,}\")\n",
    "print(f\"–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {df_likes['user_id'].nunique():,}\")\n",
    "\n",
    "# 2. –¢–û–õ–¨–ö–û –í–†–ï–ú–ï–ù–ù–û–ô –°–ü–õ–ò–¢ (—É–¥–∞–ª–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π —Å–ø–ª–∏—Ç –ø–æ ID)\n",
    "min_time = df_likes['timestamp'].min()\n",
    "max_time = df_likes['timestamp'].max()\n",
    "split_time = min_time + (max_time - min_time) * 0.8\n",
    "\n",
    "print(f\"\\nSplit –≤—Ä–µ–º—è: {split_time}\")\n",
    "print(f\"Train –ø–µ—Ä–∏–æ–¥: {min_time} - {split_time}\")\n",
    "print(f\"Test –ø–µ—Ä–∏–æ–¥: {split_time} - {max_time}\")\n",
    "\n",
    "# 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¢–û–õ–¨–ö–û –ø–æ –≤—Ä–µ–º–µ–Ω–∏\n",
    "df_train = df_likes[df_likes['timestamp'] < split_time].copy()\n",
    "df_test = df_likes[df_likes['timestamp'] >= split_time].copy()\n",
    "\n",
    "print(f\"\\nTrain –ª–∞–π–∫–æ–≤: {len(df_train):,}\")\n",
    "print(f\"Test –ª–∞–π–∫–æ–≤: {len(df_test):,}\")\n",
    "\n",
    "# 4. –ò—Å–∫–ª—é—á–∞–µ–º cold-start –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ test (—Ç–µ—Ö, –∫–æ–≥–æ –Ω–µ—Ç –≤ train)\n",
    "train_user_set = set(df_train['user_id'].unique())\n",
    "df_test = df_test[df_test['user_id'].isin(train_user_set)].copy()\n",
    "\n",
    "# 5. –ò—Å–∫–ª—é—á–∞–µ–º cold-start –ø–æ—Å—Ç—ã –∏–∑ test (—Ç–µ—Ö, –∫–æ–≥–æ –Ω–µ—Ç –≤ train)\n",
    "train_post_set = set(df_train['post_id'].unique())\n",
    "df_test = df_test[df_test['post_id'].isin(train_post_set)].copy()\n",
    "\n",
    "print(f\"Test –ª–∞–π–∫–æ–≤ –ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è cold-start: {len(df_test):,}\")\n",
    "\n",
    "# 6. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è ALS (—Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö)\n",
    "train_users_sorted = np.sort(df_train['user_id'].unique())\n",
    "train_posts_sorted = np.sort(df_train['post_id'].unique())\n",
    "\n",
    "user_to_idx = {uid: i for i, uid in enumerate(train_users_sorted)}\n",
    "post_to_idx = {pid: i for i, pid in enumerate(train_posts_sorted)}\n",
    "\n",
    "rows = [user_to_idx[u] for u in df_train['user_id']]\n",
    "cols = [post_to_idx[p] for p in df_train['post_id']]\n",
    "\n",
    "train_matrix = sparse.csr_matrix(\n",
    "    (np.ones(len(df_train), dtype=np.float32), (rows, cols)),\n",
    "    shape=(len(user_to_idx), len(post_to_idx))\n",
    ")\n",
    "\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä train –º–∞—Ç—Ä–∏—Ü—ã: {train_matrix.shape}\")\n",
    "print(f\"–ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å: {train_matrix.nnz / (train_matrix.shape[0] * train_matrix.shape[1]) * 100:.4f}%\")\n",
    "\n",
    "# 7. –û–±—É—á–∞–µ–º ALS\n",
    "print(\"\\n–û–±—É—á–µ–Ω–∏–µ ALS...\")\n",
    "model = AlternatingLeastSquares(\n",
    "    factors=20,\n",
    "    iterations=15,\n",
    "    regularization=0.01,\n",
    "    alpha=40,\n",
    "    random_state=42,\n",
    "    use_gpu=False\n",
    ")\n",
    "model.fit(train_matrix, show_progress=True)\n",
    "\n",
    "# 8. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã\n",
    "def normalize(v):\n",
    "    norms = np.linalg.norm(v, axis=1, keepdims=True)\n",
    "    return v / np.clip(norms, 1e-10, None)\n",
    "\n",
    "user_factors = normalize(model.user_factors)\n",
    "post_factors = normalize(model.item_factors)\n",
    "\n",
    "# 9. –°–û–•–†–ê–ù–Ø–ï–ú –§–ê–ö–¢–û–†–´ –¢–û–õ–¨–ö–û –î–õ–Ø TRAIN –û–ë–™–ï–ö–¢–û–í (–±–µ–∑ —Å—Ä–µ–¥–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è test!)\n",
    "# –≠—Ç–æ –∫–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: —Ñ–∞–∫—Ç–æ—Ä—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Ö, –∫—Ç–æ –±—ã–ª –≤ train\n",
    "np.save('user_factors_train.npy', user_factors.astype(np.float32))\n",
    "np.save('post_factors_train.npy', post_factors.astype(np.float32))\n",
    "\n",
    "with open('als_mappings_train.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'user_to_idx': user_to_idx,        # –¢–æ–ª—å–∫–æ train –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏\n",
    "        'post_to_idx': post_to_idx,        # –¢–æ–ª—å–∫–æ train –ø–æ—Å—Ç—ã\n",
    "        'split_time': split_time,          # –í—Ä–µ–º—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è\n",
    "        'train_users_count': len(train_users_sorted),\n",
    "        'train_posts_count': len(train_posts_sorted)\n",
    "    }, f)\n",
    "\n",
    "df_test.to_csv('test_data_correct.csv', index=False)\n",
    "\n",
    "# 10. –ò—Ç–æ–≥\n",
    "print(f\"\\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –ë–ï–ó –£–¢–ï–ß–ï–ö!\")\n",
    "print(f\"   User factors: {user_factors.shape} (—Ç–æ–ª—å–∫–æ –¥–ª—è {len(train_users_sorted):,} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ train)\")\n",
    "print(f\"   Post factors: {post_factors.shape} (—Ç–æ–ª—å–∫–æ –¥–ª—è {len(train_posts_sorted):,} –ø–æ—Å—Ç–æ–≤ –∏–∑ train)\")\n",
    "print(f\"   Train –ª–∞–π–∫–æ–≤: {len(df_train):,}\")\n",
    "print(f\"   Test –ª–∞–π–∫–æ–≤: {len(df_test):,} (—Ç–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏ –ø–æ—Å—Ç—ã –∏–∑ train)\")\n",
    "print(f\"\\nüìå –¢–µ–ø–µ—Ä—å –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LGBM:\")\n",
    "print(f\"   - –î–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ train –ø–µ—Ä–∏–æ–¥–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–∏ —Ñ–∞–∫—Ç–æ—Ä—ã\")\n",
    "print(f\"   - –î–ª—è –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–ø–æ—Å–ª–µ split_time): —Ñ–∞–∫—Ç–æ—Ä—ã = 0 –∏–ª–∏ NaN\")\n",
    "print(f\"   - –≠—Ç–æ—Ç –∂–µ split_time –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è LGBM –¥–∞—Ç–∞—Å–µ—Ç–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a47a2-c4db-4cc7-9985-a2c7133bb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes = pd.read_csv('all_likes_df.csv')\n",
    "df_likes['timestamp'] = pd.to_datetime(df_likes['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d2e336b-fa06-4c97-bc46-8e0db1fecb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–ª—é—á–∏ –≤ mappings:\n",
      "['user_to_idx', 'post_to_idx', 'split_time', 'train_users_count', 'train_posts_count']\n",
      "\n",
      "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ mappings:\n",
      "\n",
      "user_to_idx: <class 'dict'>\n",
      "  –î–ª–∏–Ω–∞: 163168\n",
      "  –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: [(200, 0), (201, 1), (202, 2)]\n",
      "\n",
      "post_to_idx: <class 'dict'>\n",
      "  –î–ª–∏–Ω–∞: 6831\n",
      "  –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: [(1, 0), (2, 1), (3, 2)]\n",
      "\n",
      "split_time: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "  –ó–Ω–∞—á–µ–Ω–∏–µ: 2021-12-12 00:59:05.600000\n",
      "\n",
      "train_users_count: <class 'int'>\n",
      "  –ó–Ω–∞—á–µ–Ω–∏–µ: 163168\n",
      "\n",
      "train_posts_count: <class 'int'>\n",
      "  –ó–Ω–∞—á–µ–Ω–∏–µ: 6831\n"
     ]
    }
   ],
   "source": [
    "with open('als_mappings_train.pkl', 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "print(\"–ö–ª—é—á–∏ –≤ mappings:\")\n",
    "print(list(mappings.keys()))\n",
    "\n",
    "# –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ–ª–Ω–æ—Å—Ç—å—é:\n",
    "print(\"\\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ mappings:\")\n",
    "for key, value in mappings.items():\n",
    "    print(f\"\\n{key}: {type(value)}\")\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  –î–ª–∏–Ω–∞: {len(value)}\")\n",
    "        print(f\"  –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {list(value.items())[:3]}\")\n",
    "    else:\n",
    "        print(f\"  –ó–Ω–∞—á–µ–Ω–∏–µ: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ffc1593-e23c-4180-8ce7-4b3c11f53779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "Features: 2,306,255 –∑–∞–ø–∏—Å–µ–π (22.0 MB)\n",
      "–°–æ–∑–¥–∞–µ–º post_to_group mapping...\n",
      "\n",
      "2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\n",
      "  User-group stats...\n",
      "  Global group stats...\n",
      "  User view counts...\n",
      "  Seen posts...\n",
      "  Liked posts from df_likes...\n",
      "  Loaded 163,202 users from df_likes\n",
      "\n",
      "3. –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...\n",
      "–ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: 6831 –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 20\n",
      "\n",
      "============================================================\n",
      "–ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò\n",
      "============================================================\n",
      "–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: 163,168\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º k=300 (FAISS –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å 300 –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)\n",
      "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 10000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [14:30<00:00, 51.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\n",
      "‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è 163,168 —é–∑–µ—Ä–æ–≤\n",
      "\n",
      "========================================\n",
      "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
      "========================================\n",
      "–°—Ä–µ–¥–Ω–µ–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —é–∑–µ—Ä–∞: 30.0\n",
      "–ú–µ–¥–∏–∞–Ω–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: 30\n",
      "–ú–∏–Ω–∏–º—É–º: 30\n",
      "–ú–∞–∫—Å–∏–º—É–º: 30\n",
      "\n",
      "–Æ–∑–µ—Ä–æ–≤ —Å ‚â•20 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: 163,168 (100.0%)\n",
      "–Æ–∑–µ—Ä–æ–≤ —Å 30 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: 163,168 (100.0%)\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –¥–ª—è —é–∑–µ—Ä–∞ 200:\n",
      "  –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: 30\n",
      "  –ü–µ—Ä–≤—ã–µ 3: [4049, 6582, 3503]\n"
     ]
    }
   ],
   "source": [
    "### –ù–û–í–´–ô –°–ö–†–ò–ü–¢ –ü–û –°–û–ó–î–ê–ù–ò–Æ –°–õ–û–í–ê–†–Ø –ü–û–°–¢–û–í-–ö–ê–ù–î–ò–î–ê–¢–û–í\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import pickle\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –í–∫–ª—é—á–∏—Ç—å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å FAISS\n",
    "faiss.omp_set_num_threads(8)\n",
    "\n",
    "# ==================== 1. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê ====================\n",
    "print(\"1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –ª–∞–π–∫–æ–≤\n",
    "df_likes = pd.read_csv('all_likes_df.csv')\n",
    "df_likes['timestamp'] = pd.to_datetime(df_likes['timestamp'])\n",
    "\n",
    "features_df = pd.read_csv(\n",
    "    'df_final_clean.csv',\n",
    "    usecols=['user_id', 'post_id', 'topic_code', 'target'],\n",
    "    dtype={\n",
    "        'user_id': 'int32',\n",
    "        'post_id': 'int32',\n",
    "        'topic_code': 'int8',\n",
    "        'target': 'bool'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Features: {len(features_df):,} –∑–∞–ø–∏—Å–µ–π ({features_df.memory_usage().sum()/1024**2:.1f} MB)\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º ALS —Ñ–∞–∫—Ç–æ—Ä—ã (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏)\n",
    "post_factors = np.load('post_factors_train.npy').astype('float32')\n",
    "user_factors = np.load('user_factors_train.npy').astype('float32')\n",
    "\n",
    "with open('als_mappings_train.pkl', 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "user_id_to_idx = mappings['user_to_idx']\n",
    "post_id_to_idx = mappings['post_to_idx']\n",
    "# –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ mapping –≤—Ä—É—á–Ω—É—é\n",
    "user_idx_to_id = {v: k for k, v in user_id_to_idx.items()}\n",
    "post_idx_to_id = {v: k for k, v in post_id_to_idx.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –±—ã—Å—Ç—Ä—ã–µ reverse mapping –º–∞—Å—Å–∏–≤—ã\n",
    "user_ids_arr = np.array(list(user_id_to_idx.keys()), dtype=np.int32)\n",
    "user_indices_arr = np.array(list(user_id_to_idx.values()), dtype=np.int32)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å post_id -> post_group –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞\n",
    "print(\"–°–æ–∑–¥–∞–µ–º post_to_group mapping...\")\n",
    "post_to_group_dict = dict(zip(features_df['post_id'], features_df['topic_code']))\n",
    "\n",
    "# ==================== 2. –°–£–ü–ï–†–ë–´–°–¢–†–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ò ====================\n",
    "print(\"\\n2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\")\n",
    "\n",
    "# User-group —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - –°–†–ê–ó–£ –í –°–õ–û–í–ê–†–¨ (–º–∏–Ω—É—è pandas)\n",
    "print(\"  User-group stats...\")\n",
    "user_group_dict = {}\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º groupby –Ω–æ —Å—Ä–∞–∑—É –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dict\n",
    "grouped = features_df.groupby(['user_id', 'topic_code'])\n",
    "for (user_id, post_group), group in grouped:\n",
    "    views = len(group)\n",
    "    likes = group['target'].sum()\n",
    "    ctr = likes / views if views > 0 else 0.0\n",
    "    \n",
    "    user_group_dict[(user_id, post_group)] = {\n",
    "        'views': int(views),\n",
    "        'likes': int(likes),\n",
    "        'ctr': ctr\n",
    "    }\n",
    "\n",
    "# Global group —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - —Ç–æ–∂–µ —Å—Ä–∞–∑—É –≤ dict\n",
    "print(\"  Global group stats...\")\n",
    "global_group_dict = {}\n",
    "\n",
    "global_grouped = features_df.groupby('topic_code')['target']\n",
    "for post_group, targets in global_grouped:\n",
    "    views = len(targets)\n",
    "    likes = targets.sum()\n",
    "    ctr = likes / views if views > 0 else 0.05\n",
    "    \n",
    "    global_group_dict[post_group] = {\n",
    "        'ctr': ctr,\n",
    "        'ctr_tanh': np.tanh(ctr * 3) * 0.4,\n",
    "        'views': int(views),\n",
    "        'likes': int(likes)\n",
    "    }\n",
    "\n",
    "# User view counts (–¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞)\n",
    "print(\"  User view counts...\")\n",
    "user_view_counts_dict = features_df.groupby('user_id').size().to_dict()\n",
    "\n",
    "# Seen –ø–æ—Å—Ç—ã (–±—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø)\n",
    "print(\"  Seen posts...\")\n",
    "user_seen_dict = features_df.groupby('user_id')['post_id'].agg(set).to_dict()\n",
    "\n",
    "# Liked –ø–æ—Å—Ç—ã –∏–∑ df_likes (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ df_likes —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\n",
    "print(\"  Liked posts from df_likes...\")\n",
    "if 'df_likes' in locals() or 'df_likes' in globals():\n",
    "    user_liked_dict = df_likes.groupby('user_id')['post_id'].agg(set).to_dict()\n",
    "    print(f\"  Loaded {len(user_liked_dict):,} users from df_likes\")\n",
    "else:\n",
    "    # Fallback: —Å–æ–∑–¥–∞–µ–º –∏–∑ features_df –µ—Å–ª–∏ df_likes –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
    "    print(\"  WARNING: df_likes not found, using features_df fallback\")\n",
    "    liked_df = features_df[features_df['target'] == True]\n",
    "    user_liked_dict = liked_df.groupby('user_id')['post_id'].agg(set).to_dict()\n",
    "    del liked_df\n",
    "\n",
    "# –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "del features_df\n",
    "gc.collect()\n",
    "\n",
    "# ==================== 3. –°–û–ó–î–ê–ù–ò–ï FAISS –ò–ù–î–ï–ö–°–ê ====================\n",
    "print(\"\\n3. –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...\")\n",
    "dimension = post_factors.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è cosine similarity\n",
    "faiss_index.add(post_factors)\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {dimension}\")\n",
    "\n",
    "# ==================== 4. –£–ü–†–û–©–ï–ù–ù–ê–Ø –ò –ë–´–°–¢–†–ê–Ø –§–£–ù–ö–¶–ò–Ø –†–ê–°–ß–ï–¢–ê SCORE ====================\n",
    "\n",
    "def calculate_novelty_normalized(post_id, seen_set, min_post_id, max_post_id):\n",
    "    if post_id in seen_set:\n",
    "        return 0.8  # –ë—ã–ª–æ 0.3, —Å—Ç–∞–ª–æ 0.8 (–º–µ–Ω—å—à–µ —à—Ç—Ä–∞—Ñ)\n",
    "    \n",
    "    novelty_norm = (post_id - min_post_id) / (max_post_id - min_post_id + 1e-8)\n",
    "    return 1.0 + novelty_norm * 0.2  # –ë—ã–ª–æ 1.0, —Å—Ç–∞–ª–æ 0.2 (–º–µ–Ω—å—à–µ –±–æ–Ω—É—Å)\n",
    "\n",
    "\n",
    "def calculate_scores_with_novelty(cosines, post_ids, user_id, seen_set, liked_set):\n",
    "    \"\"\"–†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∞ —Å —É—á–µ—Ç–æ–º –Ω–æ–≤–∏–∑–Ω—ã —á–µ—Ä–µ–∑ post_id.\"\"\"\n",
    "    n = len(post_ids)\n",
    "    scores = cosines.copy()\n",
    "    \n",
    "    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω/–º–∞–∫—Å ID –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "    min_id = min(post_ids)\n",
    "    max_id = max(post_ids)\n",
    "    \n",
    "    for i in range(n):\n",
    "        post_id = post_ids[i]\n",
    "        cosine = cosines[i]\n",
    "        \n",
    "        # 1. –ë–æ–Ω—É—Å/—à—Ç—Ä–∞—Ñ –∑–∞ –Ω–æ–≤–∏–∑–Ω—É (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä!)\n",
    "        novelty_bonus = calculate_novelty_normalized(\n",
    "            post_id, seen_set, min_id, max_id\n",
    "        )\n",
    "        scores[i] *= novelty_bonus\n",
    "        \n",
    "        # 2. Personal CTR (–≤—Ç–æ—Ä–∏—á–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä)\n",
    "        post_group = post_to_group_dict.get(post_id, 45)\n",
    "        key = (user_id, post_group)\n",
    "        \n",
    "        if key in user_group_dict:\n",
    "            stats = user_group_dict[key]\n",
    "            if stats['views'] >= 3 and stats['ctr'] > 0.1:\n",
    "                # –ê–¥–¥–∏—Ç–∏–≤–Ω—ã–π –±–æ–Ω—É—Å, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π\n",
    "                ctr_bonus = min(0.3, stats['ctr'] * 0.7)\n",
    "                scores[i] += ctr_bonus\n",
    "        \n",
    "        # 3. Global CTR (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ)\n",
    "        if post_group in global_group_dict:\n",
    "            g_bonus = global_group_dict[post_group]['ctr_tanh'] * 0.05\n",
    "            scores[i] += g_bonus\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# ==================== 5. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ë–ê–¢–ß–ï–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê ====================\n",
    "def process_batch_fast(user_batch, k=300):\n",
    "    \"\"\"–°—É–ø–µ—Ä–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞.\"\"\"\n",
    "    batch_results = {}\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏\n",
    "    valid_users = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for uid in user_batch:\n",
    "        if uid in user_id_to_idx:\n",
    "            valid_users.append(uid)\n",
    "            valid_indices.append(user_id_to_idx[uid])\n",
    "    \n",
    "    if not valid_users:\n",
    "        return {uid: [] for uid in user_batch}\n",
    "    \n",
    "    # FAISS –ø–æ–∏—Å–∫ –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "    user_vectors = user_factors[valid_indices]\n",
    "    D_batch, I_batch = faiss_index.search(user_vectors, k=k)\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    for i, user_id in enumerate(valid_users):\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\n",
    "        cosines = D_batch[i]\n",
    "        post_indices = I_batch[i]\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ post_id\n",
    "        candidate_post_ids = []\n",
    "        for idx in post_indices:\n",
    "            if idx < len(post_idx_to_id):\n",
    "                candidate_post_ids.append(post_idx_to_id[idx])\n",
    "        \n",
    "        if not candidate_post_ids:\n",
    "            batch_results[user_id] = []\n",
    "            continue\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º –ª–∞–π–∫–Ω—É—Ç—ã–µ (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)\n",
    "        liked_set = user_liked_dict.get(user_id, set())\n",
    "        seen_set = user_seen_dict.get(user_id, set())\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "        filtered_post_ids = []\n",
    "        filtered_cosines = []\n",
    "        \n",
    "        for post_id, cosine in zip(candidate_post_ids, cosines):\n",
    "            if post_id not in liked_set:\n",
    "                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "                if post_id in seen_set and cosine < 0.6:\n",
    "                    continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∞–±–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ\n",
    "                filtered_post_ids.append(post_id)\n",
    "                filtered_cosines.append(cosine)\n",
    "        \n",
    "        if not filtered_post_ids:\n",
    "            batch_results[user_id] = []\n",
    "            continue\n",
    "        \n",
    "        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∞\n",
    "        scores = calculate_scores_with_novelty(\n",
    "            np.array(filtered_cosines, dtype=np.float32),\n",
    "            filtered_post_ids,\n",
    "            user_id,\n",
    "            seen_set,\n",
    "            liked_set\n",
    "        )\n",
    "        \n",
    "        # –ë—ã—Å—Ç—Ä–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ç–æ–ø-N\n",
    "        top_n = 30\n",
    "        if len(scores) > top_n:\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º argpartition –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–æ–ø-N\n",
    "            top_indices = np.argpartition(-scores, top_n)[:top_n]\n",
    "            top_scores = scores[top_indices]\n",
    "            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N\n",
    "            sorted_top_indices = top_indices[np.argsort(-top_scores)]\n",
    "            top_post_ids = [filtered_post_ids[idx] for idx in sorted_top_indices]\n",
    "        else:\n",
    "            # –ï—Å–ª–∏ –º–∞–ª–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –ø—Ä–æ—Å—Ç–æ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ\n",
    "            sorted_indices = np.argsort(-scores)\n",
    "            top_post_ids = [filtered_post_ids[idx] for idx in sorted_indices]\n",
    "        \n",
    "        batch_results[user_id] = top_post_ids\n",
    "    \n",
    "    # –î–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –±–µ–∑ —Ñ–∞–∫—Ç–æ—Ä–æ–≤\n",
    "    for uid in user_batch:\n",
    "        if uid not in batch_results:\n",
    "            batch_results[uid] = []\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# ==================== 6. –ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# –í—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏\n",
    "all_user_ids = np.array(list(user_id_to_idx.keys()), dtype=np.int32)\n",
    "n_users = len(all_user_ids)\n",
    "print(f\"–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {n_users:,}\")\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞—Ç—á–∏–Ω–≥–∞\n",
    "batch_size = 10000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞—Ç—á –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "all_results = {}\n",
    "\n",
    "# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π k –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è (6800 –ø–æ—Å—Ç–æ–≤)\n",
    "k_optimal = 300  # 4.4% –æ—Ç –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤ - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ!\n",
    "\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º k={k_optimal} (FAISS –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å {k_optimal} –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\")\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏\n",
    "n_batches = (n_users + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in tqdm(range(0, n_users, batch_size), total=n_batches, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π\"):\n",
    "    batch_end = min(batch_idx + batch_size, n_users)\n",
    "    user_batch = all_user_ids[batch_idx:batch_end]\n",
    "    \n",
    "    batch_results = process_batch_fast(user_batch, k=k_optimal)\n",
    "    all_results.update(batch_results)\n",
    "    \n",
    "    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ 50K –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "    if batch_idx % 50000 == 0 and batch_idx > 0:\n",
    "        gc.collect()\n",
    "\n",
    "# ==================== 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================\n",
    "print(\"\\n7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ pickle\n",
    "with open('precomputed_candidates_fast_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è {len(all_results):,} —é–∑–µ—Ä–æ–≤\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "result_lengths = [len(posts) for posts in all_results.values()]\n",
    "print(f\"–°—Ä–µ–¥–Ω–µ–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —é–∑–µ—Ä–∞: {np.mean(result_lengths):.1f}\")\n",
    "print(f\"–ú–µ–¥–∏–∞–Ω–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {np.median(result_lengths):.0f}\")\n",
    "print(f\"–ú–∏–Ω–∏–º—É–º: {min(result_lengths)}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º—É–º: {max(result_lengths)}\")\n",
    "\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É\n",
    "counts_20_plus = sum(1 for x in result_lengths if x >= 20)\n",
    "counts_30 = sum(1 for x in result_lengths if x == 30)\n",
    "print(f\"\\n–Æ–∑–µ—Ä–æ–≤ —Å ‚â•20 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: {counts_20_plus:,} ({counts_20_plus/n_users*100:.1f}%)\")\n",
    "print(f\"–Æ–∑–µ—Ä–æ–≤ —Å 30 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: {counts_30:,} ({counts_30/n_users*100:.1f}%)\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —é–∑–µ—Ä–∞\n",
    "if all_results:\n",
    "    first_user = list(all_results.keys())[0]\n",
    "    print(f\"\\n–ü—Ä–∏–º–µ—Ä –¥–ª—è —é–∑–µ—Ä–∞ {first_user}:\")\n",
    "    print(f\"  –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_results[first_user])}\")\n",
    "    if len(all_results[first_user]) > 0:\n",
    "        print(f\"  –ü–µ—Ä–≤—ã–µ 3: {all_results[first_user][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f4866-47d5-4263-b9f5-2a8acac2ab7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6acf335e-37f1-4a51-a542-3de331c77913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DataFrame\n",
    "sql_data = []\n",
    "for user_id, post_ids in all_results.items():\n",
    "    for position, post_id in enumerate(post_ids, 1):  # position –æ—Ç 1 –¥–æ N\n",
    "        sql_data.append({\n",
    "            'user_id': int(user_id),\n",
    "            'post_id': int(post_id),\n",
    "            'position': position,  # –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (1 - –ª—É—á—à–∏–π)\n",
    "        })\n",
    "\n",
    "df_sql_cand = pd.DataFrame(sql_data)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV –¥–ª—è COPY\n",
    "df_sql_cand.to_csv('recommendations_3.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e3651a-d6f5-4914-b273-e25033c9f709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>4049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>6582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>3503</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  post_id  position\n",
       "0      200     4049         1\n",
       "1      200     6582         2\n",
       "2      200     3503         3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql_cand.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a7c0bf-d10b-4335-99d6-65ab9b12e395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>4049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>6582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>3503</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  post_id  position\n",
       "0      200     4049         1\n",
       "1      200     6582         2\n",
       "2      200     3503         3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql_cand.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73af309a-ad87-44e4-a516-a394a6cba7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4895040 entries, 0 to 4895039\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype\n",
      "---  ------    -----\n",
      " 0   user_id   int64\n",
      " 1   post_id   int64\n",
      " 2   position  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 112.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_sql_cand.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0f179e-ede9-4368-a32e-d7e704f2f55d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: 4,895,040, —á–∞–Ω–∫–æ–≤: 10\n",
      "‚úÖ –ß–∞–Ω–∫ 1/10 –∑–∞–ø–∏—Å–∞–Ω (500,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 60.2 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 10.2%\n",
      "‚úÖ –ß–∞–Ω–∫ 2/10 –∑–∞–ø–∏—Å–∞–Ω (1,000,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 40.7 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 20.4%\n",
      "‚úÖ –ß–∞–Ω–∫ 3/10 –∑–∞–ø–∏—Å–∞–Ω (1,500,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 45.1 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 30.6%\n",
      "‚úÖ –ß–∞–Ω–∫ 4/10 –∑–∞–ø–∏—Å–∞–Ω (2,000,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 41.6 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 40.9%\n",
      "‚úÖ –ß–∞–Ω–∫ 5/10 –∑–∞–ø–∏—Å–∞–Ω (2,500,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 41.5 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 51.1%\n",
      "‚úÖ –ß–∞–Ω–∫ 6/10 –∑–∞–ø–∏—Å–∞–Ω (3,000,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 37.4 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 61.3%\n",
      "‚úÖ –ß–∞–Ω–∫ 7/10 –∑–∞–ø–∏—Å–∞–Ω (3,500,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 41.3 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 71.5%\n",
      "‚úÖ –ß–∞–Ω–∫ 8/10 –∑–∞–ø–∏—Å–∞–Ω (4,000,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 40.4 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 81.7%\n",
      "‚úÖ –ß–∞–Ω–∫ 9/10 –∑–∞–ø–∏—Å–∞–Ω (4,500,000/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 44.1 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 91.9%\n",
      "‚úÖ –ß–∞–Ω–∫ 10/10 –∑–∞–ø–∏—Å–∞–Ω (4,895,040/4,895,040 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 36.1 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 100.0%\n",
      "\n",
      "üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n",
      "‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: 7.3 –º–∏–Ω—É—Ç\n",
      "üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: 11161 —Å—Ç—Ä–æ–∫/—Å–µ–∫\n",
      "üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "### –ó–ê–ì–†–£–ó–ö–ê –ü–û–°–¢–û–í-–ö–ê–ù–î–ò–î–ê–¢–û–í –í SQL\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "def safe_chunked_upload(df, table_name, chunk_size=500000):\n",
    "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    total_chunks = total_rows // chunk_size + 1\n",
    "    print(f\"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows:,}, —á–∞–Ω–∫–æ–≤: {total_chunks}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, start in enumerate(range(0, total_rows, chunk_size)):\n",
    "        chunk_start_time = time.time()\n",
    "        end = min(start + chunk_size, total_rows)\n",
    "        chunk = df.iloc[start:end]\n",
    "        \n",
    "        # –ö–ê–ñ–î–´–ô –†–ê–ó —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "        engine = create_engine(\n",
    "            \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "            \"postgres.lab.karpov.courses:6432/startml\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            if start == 0:\n",
    "                # –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ - —Å–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É\n",
    "                chunk.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "            else:\n",
    "                # –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ - –¥–æ–±–∞–≤–ª—è–µ–º\n",
    "                chunk.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "            \n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            \n",
    "            print(f\"‚úÖ –ß–∞–Ω–∫ {i+1}/{total_chunks} –∑–∞–ø–∏—Å–∞–Ω ({end:,}/{total_rows:,} —Å—Ç—Ä–æ–∫)\")\n",
    "            print(f\"   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: {chunk_time:.1f} —Å–µ–∫\")\n",
    "            print(f\"   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {(end/total_rows*100):.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ —á–∞–Ω–∫–µ {i+1}: {e}\")\n",
    "            return False\n",
    "        \n",
    "        finally:\n",
    "            # –í–°–ï–ì–î–ê –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "            engine.dispose()\n",
    "            time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏\n",
    "    \n",
    "    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
    "    print(f\"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç\")\n",
    "    print(f\"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {total_rows/total_time:.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "if safe_chunked_upload(df_sql_cand, 'g_seregin_lbm4969_precomputed_candidates', 500000):\n",
    "    print(\"üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã!\")\n",
    "else:\n",
    "    print(\"üí• –ó–∞–ø–∏—Å—å –ø—Ä–µ—Ä–≤–∞–Ω–∞ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f3d34-cdcb-4f5a-94b3-c829fa535ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f19c8e-e9cc-4a71-b7f2-21e9cf806cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07361d-8ab2-4675-8d3a-4e5d46c22e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d669973-9ce3-4d29-9b8f-ede53e56ed7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de693eee-7abf-49d8-8f47-88798d9f9ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22991b6d-6199-4e02-b067-f8fd0a430e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba42b7e-08e9-48d7-b4f3-b004176899cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f77ab2-eead-43db-aef8-49e569cce2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bae47e-0d1c-42c8-bfe2-da0736d40db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010137c4-2b08-452b-ad9b-ba7aef181f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85efafc6-4d8e-4536-b7ee-0b88122417fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61938e0-1ab8-49db-b8a1-9b5c36a188cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "25528a9e-0d9c-49c9-a1da-65c3195adb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "Features: 2,193,311 –∑–∞–ø–∏—Å–µ–π (20.9 MB)\n",
      "–°–æ–∑–¥–∞–µ–º post_to_group mapping...\n",
      "\n",
      "2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\n",
      "  User-group stats...\n",
      "  Global group stats...\n",
      "  User view counts...\n",
      "  Seen/liked posts...\n",
      "\n",
      "3. –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...\n",
      "–ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: 6831 –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 20\n",
      "\n",
      "============================================================\n",
      "–ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò\n",
      "============================================================\n",
      "–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: 163,202\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º k=300 (FAISS –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å 300 –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)\n",
      "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 10000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 0/163,202 (0.0%)\n",
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 50,000/163,202 (30.6%)\n",
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 100,000/163,202 (61.3%)\n",
      "–ü—Ä–æ–≥—Ä–µ—Å—Å: 150,000/163,202 (91.9%)\n",
      "\n",
      "7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\n",
      "‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è 163,202 —é–∑–µ—Ä–æ–≤\n",
      "\n",
      "========================================\n",
      "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
      "========================================\n",
      "–°—Ä–µ–¥–Ω–µ–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —é–∑–µ—Ä–∞: 30.0\n",
      "–ú–µ–¥–∏–∞–Ω–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: 30\n",
      "–ú–∏–Ω–∏–º—É–º: 30\n",
      "–ú–∞–∫—Å–∏–º—É–º: 30\n",
      "\n",
      "–Æ–∑–µ—Ä–æ–≤ —Å ‚â•30 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: 163,202 (100.0%)\n",
      "–Æ–∑–µ—Ä–æ–≤ —Å 50 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: 163,202 (100.0%)\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –¥–ª—è —é–∑–µ—Ä–∞ 201:\n",
      "  –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: 30\n",
      "  –ü–µ—Ä–≤—ã–µ 3: [7183, 7166, 6999, 7030, 6956, 5892, 7080, 7010, 6727, 7191, 5538, 6574, 7171, 7090, 7272, 7078, 5918, 6551, 6464, 6111, 6763, 6272, 6392, 5717, 4604, 7094, 7068, 6772, 5707, 7100]\n",
      "CPU times: user 5min 51s, sys: 28.6 s, total: 6min 20s\n",
      "Wall time: 5min 59s\n"
     ]
    }
   ],
   "source": [
    "### –°–ö–†–ò–ü–¢ –ü–û –°–û–ó–î–ê–ù–ò–Æ –°–õ–û–í–ê–†–Ø –ü–û–°–¢–û–í-–ö–ê–ù–î–ò–î–ê–¢–û–í\n",
    "\n",
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import pickle\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –í–∫–ª—é—á–∏—Ç—å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å FAISS\n",
    "faiss.omp_set_num_threads(8)\n",
    "\n",
    "# ==================== 1. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê ====================\n",
    "print(\"1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "\n",
    "features_df = pd.read_csv(\n",
    "    'features_with_als.csv',\n",
    "    usecols=['user_id', 'post_id', 'post_group', 'target'],\n",
    "    dtype={\n",
    "        'user_id': 'int32',\n",
    "        'post_id': 'int32',\n",
    "        'post_group': 'int8',\n",
    "        'target': 'bool'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Features: {len(features_df):,} –∑–∞–ø–∏—Å–µ–π ({features_df.memory_usage().sum()/1024**2:.1f} MB)\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º ALS —Ñ–∞–∫—Ç–æ—Ä—ã (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏)\n",
    "post_factors = np.load('post_factors_normalized.npy').astype('float32')\n",
    "user_factors = np.load('user_factors_normalized.npy').astype('float32')\n",
    "\n",
    "with open('als_mappings.pkl', 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "user_id_to_idx = mappings['user_to_idx']\n",
    "user_idx_to_id = mappings['idx_to_user']\n",
    "post_id_to_idx = mappings['post_to_idx']\n",
    "post_idx_to_id = mappings['idx_to_post']\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –±—ã—Å—Ç—Ä—ã–µ reverse mapping –º–∞—Å—Å–∏–≤—ã\n",
    "user_ids_arr = np.array(list(user_id_to_idx.keys()), dtype=np.int32)\n",
    "user_indices_arr = np.array(list(user_id_to_idx.values()), dtype=np.int32)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å post_id -> post_group –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞\n",
    "print(\"–°–æ–∑–¥–∞–µ–º post_to_group mapping...\")\n",
    "post_to_group_dict = dict(zip(features_df['post_id'], features_df['post_group']))\n",
    "\n",
    "# ==================== 2. –°–£–ü–ï–†–ë–´–°–¢–†–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ò ====================\n",
    "print(\"\\n2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\")\n",
    "\n",
    "# User-group —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - –°–†–ê–ó–£ –í –°–õ–û–í–ê–†–¨ (–º–∏–Ω—É—è pandas)\n",
    "print(\"  User-group stats...\")\n",
    "user_group_dict = {}\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º groupby –Ω–æ —Å—Ä–∞–∑—É –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dict\n",
    "grouped = features_df.groupby(['user_id', 'post_group'])\n",
    "for (user_id, post_group), group in grouped:\n",
    "    views = len(group)\n",
    "    likes = group['target'].sum()\n",
    "    ctr = likes / views if views > 0 else 0.0\n",
    "    \n",
    "    user_group_dict[(user_id, post_group)] = {\n",
    "        'views': int(views),\n",
    "        'likes': int(likes),\n",
    "        'ctr': ctr\n",
    "    }\n",
    "\n",
    "# Global group —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - —Ç–æ–∂–µ —Å—Ä–∞–∑—É –≤ dict\n",
    "print(\"  Global group stats...\")\n",
    "global_group_dict = {}\n",
    "\n",
    "global_grouped = features_df.groupby('post_group')['target']\n",
    "for post_group, targets in global_grouped:\n",
    "    views = len(targets)\n",
    "    likes = targets.sum()\n",
    "    ctr = likes / views if views > 0 else 0.05\n",
    "    \n",
    "    global_group_dict[post_group] = {\n",
    "        'ctr': ctr,\n",
    "        'ctr_tanh': np.tanh(ctr * 3) * 0.4,\n",
    "        'views': int(views),\n",
    "        'likes': int(likes)\n",
    "    }\n",
    "\n",
    "# User view counts (–¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞)\n",
    "print(\"  User view counts...\")\n",
    "user_view_counts_dict = features_df.groupby('user_id').size().to_dict()\n",
    "\n",
    "# Seen –∏ liked –ø–æ—Å—Ç—ã (–±—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø)\n",
    "print(\"  Seen/liked posts...\")\n",
    "user_seen_dict = features_df.groupby('user_id')['post_id'].agg(set).to_dict()\n",
    "user_liked_dict = df_likes.groupby('user_id')['post_id'].agg(set).to_dict()\n",
    "\n",
    "# –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "del features_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# ==================== 3. –°–û–ó–î–ê–ù–ò–ï FAISS –ò–ù–î–ï–ö–°–ê ====================\n",
    "print(\"\\n3. –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...\")\n",
    "dimension = post_factors.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è cosine similarity\n",
    "faiss_index.add(post_factors)\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {dimension}\")\n",
    "\n",
    "\n",
    "\n",
    "# ==================== 4. –£–ü–†–û–©–ï–ù–ù–ê–Ø –ò –ë–´–°–¢–†–ê–Ø –§–£–ù–ö–¶–ò–Ø –†–ê–°–ß–ï–¢–ê SCORE ====================\n",
    "def calculate_novelty_normalized(post_id, seen_set, min_post_id, max_post_id):\n",
    "    \"\"\"\n",
    "    –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç –Ω–æ–≤–∏–∑–Ω—ã.\n",
    "    \"\"\"\n",
    "    if post_id in seen_set:\n",
    "        return 0.2  # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ\n",
    "    \n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º ID –æ—Ç 0 (—Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π) –¥–æ 1 (—Å–∞–º—ã–π –Ω–æ–≤—ã–π)\n",
    "    novelty_norm = (post_id - min_post_id) / (max_post_id - min_post_id)\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –±–æ–Ω—É—Å: –Ω–æ–≤—ã–µ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª—å—à–µ\n",
    "    bonus = 1.0 + novelty_norm * 1.0  # –æ—Ç +0% –¥–æ +100%\n",
    "    \n",
    "    return bonus\n",
    "\n",
    "\n",
    "    \n",
    "def calculate_scores_with_novelty(cosines, post_ids, user_id, seen_set, liked_set):\n",
    "    \"\"\"–†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∞ —Å —É—á–µ—Ç–æ–º –Ω–æ–≤–∏–∑–Ω—ã —á–µ—Ä–µ–∑ post_id.\"\"\"\n",
    "    n = len(post_ids)\n",
    "    scores = cosines.copy()\n",
    "    \n",
    "    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω/–º–∞–∫—Å ID –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "    min_id = min(post_ids)\n",
    "    max_id = max(post_ids)\n",
    "    \n",
    "    for i in range(n):\n",
    "        post_id = post_ids[i]\n",
    "        cosine = cosines[i]\n",
    "        \n",
    "        # 1. –ë–æ–Ω—É—Å/—à—Ç—Ä–∞—Ñ –∑–∞ –Ω–æ–≤–∏–∑–Ω—É (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä!)\n",
    "        novelty_bonus = calculate_novelty_normalized(\n",
    "            post_id, seen_set, min_id, max_id\n",
    "        )\n",
    "        scores[i] *= novelty_bonus\n",
    "        \n",
    "        # 2. Personal CTR (–≤—Ç–æ—Ä–∏—á–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä)\n",
    "        post_group = post_to_group_dict.get(post_id, 45)\n",
    "        key = (user_id, post_group)\n",
    "        \n",
    "        if key in user_group_dict:\n",
    "            stats = user_group_dict[key]\n",
    "            if stats['views'] >= 3 and stats['ctr'] > 0.1:\n",
    "                # –ê–¥–¥–∏—Ç–∏–≤–Ω—ã–π –±–æ–Ω—É—Å, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π\n",
    "                ctr_bonus = min(0.3, stats['ctr'] * 0.5)\n",
    "                scores[i] += ctr_bonus\n",
    "        \n",
    "        # 3. Global CTR (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ)\n",
    "        if post_group in global_group_dict:\n",
    "            g_bonus = global_group_dict[post_group]['ctr_tanh'] * 0.05\n",
    "            scores[i] += g_bonus\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "# ==================== 5. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ë–ê–¢–ß–ï–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê ====================\n",
    "def process_batch_fast(user_batch, k=300):\n",
    "    \"\"\"–°—É–ø–µ—Ä–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞.\"\"\"\n",
    "    batch_results = {}\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏\n",
    "    valid_users = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for uid in user_batch:\n",
    "        if uid in user_id_to_idx:\n",
    "            valid_users.append(uid)\n",
    "            valid_indices.append(user_id_to_idx[uid])\n",
    "    \n",
    "    if not valid_users:\n",
    "        return {uid: [] for uid in user_batch}\n",
    "    \n",
    "    # FAISS –ø–æ–∏—Å–∫ –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "    user_vectors = user_factors[valid_indices]\n",
    "    D_batch, I_batch = faiss_index.search(user_vectors, k=k)\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    for i, user_id in enumerate(valid_users):\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\n",
    "        cosines = D_batch[i]\n",
    "        post_indices = I_batch[i]\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ post_id\n",
    "        candidate_post_ids = []\n",
    "        for idx in post_indices:\n",
    "            if idx < len(post_idx_to_id):\n",
    "                candidate_post_ids.append(post_idx_to_id[idx])\n",
    "        \n",
    "        if not candidate_post_ids:\n",
    "            batch_results[user_id] = []\n",
    "            continue\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º –ª–∞–π–∫–Ω—É—Ç—ã–µ (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)\n",
    "        liked_set = user_liked_dict.get(user_id, set())\n",
    "        seen_set = user_seen_dict.get(user_id, set())\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "        filtered_post_ids = []\n",
    "        filtered_cosines = []\n",
    "        \n",
    "        for post_id, cosine in zip(candidate_post_ids, cosines):\n",
    "            if post_id not in liked_set:\n",
    "                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "                if post_id in seen_set and cosine < 0.6:\n",
    "                    continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∞–±–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ\n",
    "                filtered_post_ids.append(post_id)\n",
    "                filtered_cosines.append(cosine)\n",
    "        \n",
    "        if not filtered_post_ids:\n",
    "            batch_results[user_id] = []\n",
    "            continue\n",
    "        \n",
    "        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∞\n",
    "        scores = calculate_scores_with_novelty(\n",
    "            np.array(filtered_cosines, dtype=np.float32),\n",
    "            filtered_post_ids,\n",
    "            user_id,\n",
    "            seen_set,\n",
    "            liked_set\n",
    "        )\n",
    "        \n",
    "        # –ë—ã—Å—Ç—Ä–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ç–æ–ø-N\n",
    "        top_n = 30\n",
    "        if len(scores) > top_n:\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º argpartition –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–æ–ø-N\n",
    "            top_indices = np.argpartition(-scores, top_n)[:top_n]\n",
    "            top_scores = scores[top_indices]\n",
    "            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N\n",
    "            sorted_top_indices = top_indices[np.argsort(-top_scores)]\n",
    "            top_post_ids = [filtered_post_ids[idx] for idx in sorted_top_indices]\n",
    "        else:\n",
    "            # –ï—Å–ª–∏ –º–∞–ª–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –ø—Ä–æ—Å—Ç–æ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ\n",
    "            sorted_indices = np.argsort(-scores)\n",
    "            top_post_ids = [filtered_post_ids[idx] for idx in sorted_indices]\n",
    "        \n",
    "        batch_results[user_id] = top_post_ids\n",
    "    \n",
    "    # –î–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –±–µ–∑ —Ñ–∞–∫—Ç–æ—Ä–æ–≤\n",
    "    for uid in user_batch:\n",
    "        if uid not in batch_results:\n",
    "            batch_results[uid] = []\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "\n",
    "\n",
    "# ==================== 6. –ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# –í—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏\n",
    "all_user_ids = np.array(list(user_id_to_idx.keys()), dtype=np.int32)\n",
    "n_users = len(all_user_ids)\n",
    "print(f\"–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {n_users:,}\")\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞—Ç—á–∏–Ω–≥–∞\n",
    "batch_size = 10000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞—Ç—á –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "all_results = {}\n",
    "\n",
    "# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π k –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è (6800 –ø–æ—Å—Ç–æ–≤)\n",
    "k_optimal = 300  # 4.4% –æ—Ç –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤ - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ!\n",
    "\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º k={k_optimal} (FAISS –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å {k_optimal} –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\")\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏\n",
    "n_batches = (n_users + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in range(0, n_users, batch_size):\n",
    "    batch_end = min(batch_idx + batch_size, n_users)\n",
    "    user_batch = all_user_ids[batch_idx:batch_end]\n",
    "    \n",
    "    # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –≤–º–µ—Å—Ç–æ tqdm (—ç–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è)\n",
    "    if batch_idx % (batch_size * 5) == 0:\n",
    "        percent = batch_idx / n_users * 100\n",
    "        print(f\"–ü—Ä–æ–≥—Ä–µ—Å—Å: {batch_idx:,}/{n_users:,} ({percent:.1f}%)\")\n",
    "    \n",
    "    batch_results = process_batch_fast(user_batch, k=k_optimal)\n",
    "    all_results.update(batch_results)\n",
    "    \n",
    "    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ 50K –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "    if batch_idx % 50000 == 0 and batch_idx > 0:\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# ==================== 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================\n",
    "print(\"\\n7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ pickle\n",
    "with open('precomputed_candidates_fast_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è {len(all_results):,} —é–∑–µ—Ä–æ–≤\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "result_lengths = [len(posts) for posts in all_results.values()]\n",
    "print(f\"–°—Ä–µ–¥–Ω–µ–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —é–∑–µ—Ä–∞: {np.mean(result_lengths):.1f}\")\n",
    "print(f\"–ú–µ–¥–∏–∞–Ω–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {np.median(result_lengths):.0f}\")\n",
    "print(f\"–ú–∏–Ω–∏–º—É–º: {min(result_lengths)}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º—É–º: {max(result_lengths)}\")\n",
    "\n",
    "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É\n",
    "counts_20_plus = sum(1 for x in result_lengths if x >= 20)\n",
    "counts_30 = sum(1 for x in result_lengths if x == 30)\n",
    "print(f\"\\n–Æ–∑–µ—Ä–æ–≤ —Å ‚â•30 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: {counts_20_plus:,} ({counts_20_plus/n_users*100:.1f}%)\")\n",
    "print(f\"–Æ–∑–µ—Ä–æ–≤ —Å 30 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: {counts_30:,} ({counts_30/n_users*100:.1f}%)\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —é–∑–µ—Ä–∞\n",
    "if all_results:\n",
    "    first_user = list(all_results.keys())[0]\n",
    "    print(f\"\\n–ü—Ä–∏–º–µ—Ä –¥–ª—è —é–∑–µ—Ä–∞ {first_user}:\")\n",
    "    print(f\"  –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_results[first_user])}\")\n",
    "    if len(all_results[first_user]) > 0:\n",
    "        print(f\"  –ü–µ—Ä–≤—ã–µ 3: {all_results[first_user][:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54f11c3f-d868-4588-bb57-05fd258e3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DataFrame\n",
    "sql_data = []\n",
    "for user_id, post_ids in all_results.items():\n",
    "    for position, post_id in enumerate(post_ids, 1):  # position –æ—Ç 1 –¥–æ N\n",
    "        sql_data.append({\n",
    "            'user_id': int(user_id),\n",
    "            'post_id': int(post_id),\n",
    "            'position': position,  # –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (1 - –ª—É—á—à–∏–π)\n",
    "        })\n",
    "\n",
    "df_sql = pd.DataFrame(sql_data)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV –¥–ª—è COPY\n",
    "df_sql.to_csv('recommendations_2.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f34f264-0f26-4c70-8adb-60eab71d9787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4896060 entries, 0 to 4896059\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype\n",
      "---  ------    -----\n",
      " 0   user_id   int64\n",
      " 1   post_id   int64\n",
      " 2   position  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 112.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_sql.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c0d785e-96e5-442c-8b7b-17375c51af5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>7183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201</td>\n",
       "      <td>7166</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201</td>\n",
       "      <td>6999</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201</td>\n",
       "      <td>7030</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201</td>\n",
       "      <td>6956</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896055</th>\n",
       "      <td>168551</td>\n",
       "      <td>6641</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896056</th>\n",
       "      <td>168551</td>\n",
       "      <td>7266</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896057</th>\n",
       "      <td>168551</td>\n",
       "      <td>6902</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896058</th>\n",
       "      <td>168551</td>\n",
       "      <td>7125</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896059</th>\n",
       "      <td>168551</td>\n",
       "      <td>5513</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4896060 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  post_id  position\n",
       "0            201     7183         1\n",
       "1            201     7166         2\n",
       "2            201     6999         3\n",
       "3            201     7030         4\n",
       "4            201     6956         5\n",
       "...          ...      ...       ...\n",
       "4896055   168551     6641        26\n",
       "4896056   168551     7266        27\n",
       "4896057   168551     6902        28\n",
       "4896058   168551     7125        29\n",
       "4896059   168551     5513        30\n",
       "\n",
       "[4896060 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5567f799-bdc5-4be0-8866-4abca9a50b4c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: 4,896,060, —á–∞–Ω–∫–æ–≤: 10\n",
      "‚úÖ –ß–∞–Ω–∫ 1/10 –∑–∞–ø–∏—Å–∞–Ω (500,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 33.4 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 10.2%\n",
      "‚úÖ –ß–∞–Ω–∫ 2/10 –∑–∞–ø–∏—Å–∞–Ω (1,000,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 22.9 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 20.4%\n",
      "‚úÖ –ß–∞–Ω–∫ 3/10 –∑–∞–ø–∏—Å–∞–Ω (1,500,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 21.2 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 30.6%\n",
      "‚úÖ –ß–∞–Ω–∫ 4/10 –∑–∞–ø–∏—Å–∞–Ω (2,000,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 22.4 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 40.8%\n",
      "‚úÖ –ß–∞–Ω–∫ 5/10 –∑–∞–ø–∏—Å–∞–Ω (2,500,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 34.9 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 51.1%\n",
      "‚úÖ –ß–∞–Ω–∫ 6/10 –∑–∞–ø–∏—Å–∞–Ω (3,000,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 23.3 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 61.3%\n",
      "‚úÖ –ß–∞–Ω–∫ 7/10 –∑–∞–ø–∏—Å–∞–Ω (3,500,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 21.8 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 71.5%\n",
      "‚úÖ –ß–∞–Ω–∫ 8/10 –∑–∞–ø–∏—Å–∞–Ω (4,000,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 19.7 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 81.7%\n",
      "‚úÖ –ß–∞–Ω–∫ 9/10 –∑–∞–ø–∏—Å–∞–Ω (4,500,000/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 20.6 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 91.9%\n",
      "‚úÖ –ß–∞–Ω–∫ 10/10 –∑–∞–ø–∏—Å–∞–Ω (4,896,060/4,896,060 —Å—Ç—Ä–æ–∫)\n",
      "   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: 15.8 —Å–µ–∫\n",
      "   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: 100.0%\n",
      "\n",
      "üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n",
      "‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: 4.1 –º–∏–Ω—É—Ç\n",
      "üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: 19896 —Å—Ç—Ä–æ–∫/—Å–µ–∫\n",
      "üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "### –ó–ê–ì–†–£–ó–ö–ê –ü–û–°–¢–û–í-–ö–ê–ù–î–ò–î–ê–¢–û–í –í SQL\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "\n",
    "def safe_chunked_upload(df, table_name, chunk_size=500000):\n",
    "    \"\"\"–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    total_chunks = total_rows // chunk_size + 1\n",
    "    print(f\"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows:,}, —á–∞–Ω–∫–æ–≤: {total_chunks}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, start in enumerate(range(0, total_rows, chunk_size)):\n",
    "        chunk_start_time = time.time()\n",
    "        end = min(start + chunk_size, total_rows)\n",
    "        chunk = df.iloc[start:end]\n",
    "        \n",
    "        # –ö–ê–ñ–î–´–ô –†–ê–ó —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "        engine = create_engine(\n",
    "            \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "            \"postgres.lab.karpov.courses:6432/startml\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            if start == 0:\n",
    "                # –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫ - —Å–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É\n",
    "                chunk.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "            else:\n",
    "                # –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ - –¥–æ–±–∞–≤–ª—è–µ–º\n",
    "                chunk.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "            \n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            \n",
    "            print(f\"‚úÖ –ß–∞–Ω–∫ {i+1}/{total_chunks} –∑–∞–ø–∏—Å–∞–Ω ({end:,}/{total_rows:,} —Å—Ç—Ä–æ–∫)\")\n",
    "            print(f\"   ‚è±Ô∏è –í—Ä–µ–º—è —á–∞–Ω–∫–∞: {chunk_time:.1f} —Å–µ–∫\")\n",
    "            print(f\"   üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {(end/total_rows*100):.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –≤ —á–∞–Ω–∫–µ {i+1}: {e}\")\n",
    "            return False\n",
    "        \n",
    "        finally:\n",
    "            # –í–°–ï–ì–î–ê –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "            engine.dispose()\n",
    "            time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏\n",
    "    \n",
    "    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
    "    print(f\"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time/60:.1f} –º–∏–Ω—É—Ç\")\n",
    "    print(f\"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {total_rows/total_time:.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "if safe_chunked_upload(df_sql, 'g_seregin_lbm4969_precomputed_candidates_2', 500000):\n",
    "    print(\"üéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–∏—Å–∞–Ω—ã!\")\n",
    "else:\n",
    "    print(\"üí• –ó–∞–ø–∏—Å—å –ø—Ä–µ—Ä–≤–∞–Ω–∞ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df628a5-f99b-4084-a92f-8cc5fc343871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e97a18-06bf-4541-a7dd-ca4321fbb6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba5986-5ec2-4a54-9627-c2132ce11d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1fb421-8bac-4de7-83d1-aec1ac12f24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca85f2-0464-452b-9bb8-db81ba73d882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46366733-1044-4988-8c59-8dd593e1801b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbf2f7-1635-4200-8777-a19771812d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea394e59-212c-4024-85c3-ea5906f9281f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
