{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9def24-6a3d-40c4-bcdb-9d2246b17744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "\n",
    "engine = create_engine(\n",
    "    \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "    \"postgres.lab.karpov.courses:6432/startml\"\n",
    ")\n",
    "\n",
    "df_likes = pd.read_sql(\n",
    "    \" SELECT timestamp, user_id, post_id FROM  public.feed_data WHERE  target = 1;\",con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0aab8e-82b4-4969-90ab-4812cf286610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes = df_likes.sort_values('timestamp').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325813c2-f5cb-4cf8-a375-fe3dccef2e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>129394</td>\n",
       "      <td>1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>94081</td>\n",
       "      <td>5671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>59784</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>35892</td>\n",
       "      <td>4003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-01 06:01:40</td>\n",
       "      <td>8663</td>\n",
       "      <td>3837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206340</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>97105</td>\n",
       "      <td>5336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206341</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>166379</td>\n",
       "      <td>4105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206342</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>139164</td>\n",
       "      <td>3703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206343</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>38863</td>\n",
       "      <td>6839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206344</th>\n",
       "      <td>2021-12-29 23:43:27</td>\n",
       "      <td>18441</td>\n",
       "      <td>1786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8206345 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  user_id  post_id\n",
       "0       2021-10-01 06:01:40   129394     1126\n",
       "1       2021-10-01 06:01:40    94081     5671\n",
       "2       2021-10-01 06:01:40    59784      307\n",
       "3       2021-10-01 06:01:40    35892     4003\n",
       "4       2021-10-01 06:01:40     8663     3837\n",
       "...                     ...      ...      ...\n",
       "8206340 2021-12-29 23:43:27    97105     5336\n",
       "8206341 2021-12-29 23:43:27   166379     4105\n",
       "8206342 2021-12-29 23:43:27   139164     3703\n",
       "8206343 2021-12-29 23:43:27    38863     6839\n",
       "8206344 2021-12-29 23:43:27    18441     1786\n",
       "\n",
       "[8206345 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200f2548-1c91-41b8-8eca-bdf4e2e0bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes.to_csv('all_likes_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e4f973-b0d1-4c86-bfc1-402157f1c43a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgij/anaconda3/envs/my_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Å–µ–≥–æ –ª–∞–π–∫–æ–≤: 8,206,345\n",
      "–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: 6,831\n",
      "–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: 163,202\n",
      "\n",
      "Split –≤—Ä–µ–º—è: 2021-12-12 00:59:05.600000\n",
      "Train –ø–µ—Ä–∏–æ–¥: 2021-10-01 06:01:40 - 2021-12-12 00:59:05.600000\n",
      "Test –ø–µ—Ä–∏–æ–¥: 2021-12-12 00:59:05.600000 - 2021-12-29 23:43:27\n",
      "\n",
      "Train –ª–∞–π–∫–æ–≤: 6,326,477\n",
      "Test –ª–∞–π–∫–æ–≤: 1,879,868\n",
      "Test –ª–∞–π–∫–æ–≤ –ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è cold-start: 1,879,751\n",
      "\n",
      "–†–∞–∑–º–µ—Ä train –º–∞—Ç—Ä–∏—Ü—ã: (163168, 6831)\n",
      "–ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å: 0.5638%\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgij/anaconda3/envs/my_env/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 2 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:14<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –ë–ï–ó –£–¢–ï–ß–ï–ö!\n",
      "   User factors: (163168, 20) (—Ç–æ–ª—å–∫–æ –¥–ª—è 163,168 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ train)\n",
      "   Post factors: (6831, 20) (—Ç–æ–ª—å–∫–æ –¥–ª—è 6,831 –ø–æ—Å—Ç–æ–≤ –∏–∑ train)\n",
      "   Train –ª–∞–π–∫–æ–≤: 6,326,477\n",
      "   Test –ª–∞–π–∫–æ–≤: 1,879,751 (—Ç–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏ –ø–æ—Å—Ç—ã –∏–∑ train)\n",
      "\n",
      "üìå –¢–µ–ø–µ—Ä—å –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LGBM:\n",
      "   - –î–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ train –ø–µ—Ä–∏–æ–¥–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–∏ —Ñ–∞–∫—Ç–æ—Ä—ã\n",
      "   - –î–ª—è –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–ø–æ—Å–ª–µ split_time): —Ñ–∞–∫—Ç–æ—Ä—ã = 0 –∏–ª–∏ NaN\n",
      "   - –≠—Ç–æ—Ç –∂–µ split_time –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è LGBM –¥–∞—Ç–∞—Å–µ—Ç–∞!\n"
     ]
    }
   ],
   "source": [
    "# –°–ö–†–ò–ü–¢ –°–û–ó–î–ê–ù–ò–Ø –ê–õ–° –§–ê–ö–¢–û–†–û–í(c DeepSeek)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import pickle\n",
    "\n",
    "# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "df_likes = pd.read_csv('all_likes_df.csv')\n",
    "df_likes['timestamp'] = pd.to_datetime(df_likes['timestamp'])\n",
    "\n",
    "print(f\"–í—Å–µ–≥–æ –ª–∞–π–∫–æ–≤: {len(df_likes):,}\")\n",
    "print(f\"–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {df_likes['post_id'].nunique():,}\")\n",
    "print(f\"–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {df_likes['user_id'].nunique():,}\")\n",
    "\n",
    "# 2. –¢–û–õ–¨–ö–û –í–†–ï–ú–ï–ù–ù–û–ô –°–ü–õ–ò–¢ (—É–¥–∞–ª–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π —Å–ø–ª–∏—Ç –ø–æ ID)\n",
    "min_time = df_likes['timestamp'].min()\n",
    "max_time = df_likes['timestamp'].max()\n",
    "split_time = min_time + (max_time - min_time) * 0.8\n",
    "\n",
    "print(f\"\\nSplit –≤—Ä–µ–º—è: {split_time}\")\n",
    "print(f\"Train –ø–µ—Ä–∏–æ–¥: {min_time} - {split_time}\")\n",
    "print(f\"Test –ø–µ—Ä–∏–æ–¥: {split_time} - {max_time}\")\n",
    "\n",
    "# 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¢–û–õ–¨–ö–û –ø–æ –≤—Ä–µ–º–µ–Ω–∏\n",
    "df_train = df_likes[df_likes['timestamp'] < split_time].copy()\n",
    "df_test = df_likes[df_likes['timestamp'] >= split_time].copy()\n",
    "\n",
    "print(f\"\\nTrain –ª–∞–π–∫–æ–≤: {len(df_train):,}\")\n",
    "print(f\"Test –ª–∞–π–∫–æ–≤: {len(df_test):,}\")\n",
    "\n",
    "# 4. –ò—Å–∫–ª—é—á–∞–µ–º cold-start –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ test (—Ç–µ—Ö, –∫–æ–≥–æ –Ω–µ—Ç –≤ train)\n",
    "train_user_set = set(df_train['user_id'].unique())\n",
    "df_test = df_test[df_test['user_id'].isin(train_user_set)].copy()\n",
    "\n",
    "# 5. –ò—Å–∫–ª—é—á–∞–µ–º cold-start –ø–æ—Å—Ç—ã –∏–∑ test (—Ç–µ—Ö, –∫–æ–≥–æ –Ω–µ—Ç –≤ train)\n",
    "train_post_set = set(df_train['post_id'].unique())\n",
    "df_test = df_test[df_test['post_id'].isin(train_post_set)].copy()\n",
    "\n",
    "print(f\"Test –ª–∞–π–∫–æ–≤ –ø–æ—Å–ª–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è cold-start: {len(df_test):,}\")\n",
    "\n",
    "# 6. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è ALS (—Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö)\n",
    "train_users_sorted = np.sort(df_train['user_id'].unique())\n",
    "train_posts_sorted = np.sort(df_train['post_id'].unique())\n",
    "\n",
    "user_to_idx = {uid: i for i, uid in enumerate(train_users_sorted)}\n",
    "post_to_idx = {pid: i for i, pid in enumerate(train_posts_sorted)}\n",
    "\n",
    "rows = [user_to_idx[u] for u in df_train['user_id']]\n",
    "cols = [post_to_idx[p] for p in df_train['post_id']]\n",
    "\n",
    "train_matrix = sparse.csr_matrix(\n",
    "    (np.ones(len(df_train), dtype=np.float32), (rows, cols)),\n",
    "    shape=(len(user_to_idx), len(post_to_idx))\n",
    ")\n",
    "\n",
    "print(f\"\\n–†–∞–∑–º–µ—Ä train –º–∞—Ç—Ä–∏—Ü—ã: {train_matrix.shape}\")\n",
    "print(f\"–ó–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å: {train_matrix.nnz / (train_matrix.shape[0] * train_matrix.shape[1]) * 100:.4f}%\")\n",
    "\n",
    "# 7. –û–±—É—á–∞–µ–º ALS\n",
    "print(\"\\n–û–±—É—á–µ–Ω–∏–µ ALS...\")\n",
    "model = AlternatingLeastSquares(\n",
    "    factors=20,\n",
    "    iterations=15,\n",
    "    regularization=0.01,\n",
    "    alpha=40,\n",
    "    random_state=42,\n",
    "    use_gpu=False\n",
    ")\n",
    "model.fit(train_matrix, show_progress=True)\n",
    "\n",
    "# 8. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã\n",
    "def normalize(v):\n",
    "    norms = np.linalg.norm(v, axis=1, keepdims=True)\n",
    "    return v / np.clip(norms, 1e-10, None)\n",
    "\n",
    "user_factors = normalize(model.user_factors)\n",
    "post_factors = normalize(model.item_factors)\n",
    "\n",
    "# 9. –°–û–•–†–ê–ù–Ø–ï–ú –§–ê–ö–¢–û–†–´ –¢–û–õ–¨–ö–û –î–õ–Ø TRAIN –û–ë–™–ï–ö–¢–û–í (–±–µ–∑ —Å—Ä–µ–¥–Ω–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è test!)\n",
    "# –≠—Ç–æ –∫–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: —Ñ–∞–∫—Ç–æ—Ä—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Ö, –∫—Ç–æ –±—ã–ª –≤ train\n",
    "np.save('user_factors_train.npy', user_factors.astype(np.float32))\n",
    "np.save('post_factors_train.npy', post_factors.astype(np.float32))\n",
    "\n",
    "with open('als_mappings_train.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'user_to_idx': user_to_idx,        # –¢–æ–ª—å–∫–æ train –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏\n",
    "        'post_to_idx': post_to_idx,        # –¢–æ–ª—å–∫–æ train –ø–æ—Å—Ç—ã\n",
    "        'split_time': split_time,          # –í—Ä–µ–º—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è\n",
    "        'train_users_count': len(train_users_sorted),\n",
    "        'train_posts_count': len(train_posts_sorted)\n",
    "    }, f)\n",
    "\n",
    "df_test.to_csv('test_data_correct.csv', index=False)\n",
    "\n",
    "# 10. –ò—Ç–æ–≥\n",
    "print(f\"\\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –ë–ï–ó –£–¢–ï–ß–ï–ö!\")\n",
    "print(f\"   User factors: {user_factors.shape} (—Ç–æ–ª—å–∫–æ –¥–ª—è {len(train_users_sorted):,} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ train)\")\n",
    "print(f\"   Post factors: {post_factors.shape} (—Ç–æ–ª—å–∫–æ –¥–ª—è {len(train_posts_sorted):,} –ø–æ—Å—Ç–æ–≤ –∏–∑ train)\")\n",
    "print(f\"   Train –ª–∞–π–∫–æ–≤: {len(df_train):,}\")\n",
    "print(f\"   Test –ª–∞–π–∫–æ–≤: {len(df_test):,} (—Ç–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∏ –ø–æ—Å—Ç—ã –∏–∑ train)\")\n",
    "print(f\"\\nüìå –¢–µ–ø–µ—Ä—å –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è LGBM:\")\n",
    "print(f\"   - –î–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ train –ø–µ—Ä–∏–æ–¥–∞: –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç–∏ —Ñ–∞–∫—Ç–æ—Ä—ã\")\n",
    "print(f\"   - –î–ª—è –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–ø–æ—Å–ª–µ split_time): —Ñ–∞–∫—Ç–æ—Ä—ã = 0 –∏–ª–∏ NaN\")\n",
    "print(f\"   - –≠—Ç–æ—Ç –∂–µ split_time –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è LGBM –¥–∞—Ç–∞—Å–µ—Ç–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04a47a2-c4db-4cc7-9985-a2c7133bb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_likes = pd.read_csv('all_likes_df.csv')\n",
    "df_likes['timestamp'] = pd.to_datetime(df_likes['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2e336b-fa06-4c97-bc46-8e0db1fecb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–ª—é—á–∏ –≤ mappings:\n",
      "['user_to_idx', 'post_to_idx', 'split_time', 'train_users_count', 'train_posts_count']\n",
      "\n",
      "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ mappings:\n",
      "\n",
      "user_to_idx: <class 'dict'>\n",
      "  –î–ª–∏–Ω–∞: 163168\n",
      "  –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: [(200, 0), (201, 1), (202, 2)]\n",
      "\n",
      "post_to_idx: <class 'dict'>\n",
      "  –î–ª–∏–Ω–∞: 6831\n",
      "  –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: [(1, 0), (2, 1), (3, 2)]\n",
      "\n",
      "split_time: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "  –ó–Ω–∞—á–µ–Ω–∏–µ: 2021-12-12 00:59:05.600000\n",
      "\n",
      "train_users_count: <class 'int'>\n",
      "  –ó–Ω–∞—á–µ–Ω–∏–µ: 163168\n",
      "\n",
      "train_posts_count: <class 'int'>\n",
      "  –ó–Ω–∞—á–µ–Ω–∏–µ: 6831\n"
     ]
    }
   ],
   "source": [
    "with open('als_mappings_train.pkl', 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "print(\"–ö–ª—é—á–∏ –≤ mappings:\")\n",
    "print(list(mappings.keys()))\n",
    "\n",
    "# –ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ–ª–Ω–æ—Å—Ç—å—é:\n",
    "print(\"\\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ mappings:\")\n",
    "for key, value in mappings.items():\n",
    "    print(f\"\\n{key}: {type(value)}\")\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  –î–ª–∏–Ω–∞: {len(value)}\")\n",
    "        print(f\"  –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤—ã—Ö 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {list(value.items())[:3]}\")\n",
    "    else:\n",
    "        print(f\"  –ó–Ω–∞—á–µ–Ω–∏–µ: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ffc1593-e23c-4180-8ce7-4b3c11f53779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "Features: 2,316,912 –∑–∞–ø–∏—Å–µ–π (22.1 MB)\n",
      "–°–æ–∑–¥–∞–µ–º post_to_group mapping...\n",
      "\n",
      "2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\n",
      "  User-group stats...\n",
      "  Global group stats...\n",
      "  User view counts...\n",
      "  Seen posts...\n",
      "  Liked posts from df_likes...\n",
      "  Loaded 163,202 users from df_likes\n",
      "\n",
      "3. –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...\n",
      "–ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: 6831 –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 20\n",
      "\n",
      "============================================================\n",
      "–ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò\n",
      "============================================================\n",
      "–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: 163,168\n",
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º k=300 (FAISS –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å 300 –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)\n",
      "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: 10000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [09:24<00:00, 33.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\n",
      "‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è 163,168 —é–∑–µ—Ä–æ–≤\n",
      "\n",
      "========================================\n",
      "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
      "========================================\n",
      "–Æ–∑–µ—Ä–æ–≤ —Å 30 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: 163,168 (100.0%)\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä –¥–ª—è —é–∑–µ—Ä–∞ 200:\n",
      "  –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: 30\n",
      "  –ü–µ—Ä–≤—ã–µ 3: [4049, 6582, 3503]\n"
     ]
    }
   ],
   "source": [
    "### –°–ö–†–ò–ü–¢ –ü–û –°–û–ó–î–ê–ù–ò–Æ –°–õ–û–í–ê–†–Ø –ü–û–°–¢–û–í-–ö–ê–ù–î–ò–î–ê–¢–û–í(c DeepSeek)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import pickle\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# –í–∫–ª—é—á–∏—Ç—å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å FAISS\n",
    "faiss.omp_set_num_threads(8)\n",
    "\n",
    "# ==================== 1. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê ====================\n",
    "print(\"1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –ª–∞–π–∫–æ–≤\n",
    "df_likes = pd.read_csv('all_likes_df.csv')\n",
    "df_likes['timestamp'] = pd.to_datetime(df_likes['timestamp'])\n",
    "\n",
    "features_df = pd.read_csv(\n",
    "    'df_final_clean.csv',\n",
    "    usecols=['user_id', 'post_id', 'topic_code', 'target'],\n",
    "    dtype={\n",
    "        'user_id': 'int32',\n",
    "        'post_id': 'int32',\n",
    "        'topic_code': 'int8',\n",
    "        'target': 'bool'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Features: {len(features_df):,} –∑–∞–ø–∏—Å–µ–π ({features_df.memory_usage().sum()/1024**2:.1f} MB)\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º ALS —Ñ–∞–∫—Ç–æ—Ä—ã (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏)\n",
    "post_factors = np.load('post_factors_train.npy').astype('float32')\n",
    "user_factors = np.load('user_factors_train.npy').astype('float32')\n",
    "\n",
    "with open('als_mappings_train.pkl', 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "user_id_to_idx = mappings['user_to_idx']\n",
    "post_id_to_idx = mappings['post_to_idx']\n",
    "# –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ mapping –≤—Ä—É—á–Ω—É—é\n",
    "user_idx_to_id = {v: k for k, v in user_id_to_idx.items()}\n",
    "post_idx_to_id = {v: k for k, v in post_id_to_idx.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –±—ã—Å—Ç—Ä—ã–µ reverse mapping –º–∞—Å—Å–∏–≤—ã\n",
    "user_ids_arr = np.array(list(user_id_to_idx.keys()), dtype=np.int32)\n",
    "user_indices_arr = np.array(list(user_id_to_idx.values()), dtype=np.int32)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å post_id -> post_group –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞\n",
    "print(\"–°–æ–∑–¥–∞–µ–º post_to_group mapping...\")\n",
    "post_to_group_dict = dict(zip(features_df['post_id'], features_df['topic_code']))\n",
    "\n",
    "# ==================== 2. –°–£–ü–ï–†–ë–´–°–¢–†–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ò ====================\n",
    "print(\"\\n2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...\")\n",
    "\n",
    "# User-group —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - –°–†–ê–ó–£ –í –°–õ–û–í–ê–†–¨ (–º–∏–Ω—É—è pandas)\n",
    "print(\"  User-group stats...\")\n",
    "user_group_dict = {}\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º groupby –Ω–æ —Å—Ä–∞–∑—É –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dict\n",
    "grouped = features_df.groupby(['user_id', 'topic_code'])\n",
    "for (user_id, post_group), group in grouped:\n",
    "    views = len(group)\n",
    "    likes = group['target'].sum()\n",
    "    ctr = likes / views if views > 0 else 0.0\n",
    "    \n",
    "    user_group_dict[(user_id, post_group)] = {\n",
    "        'views': int(views),\n",
    "        'likes': int(likes),\n",
    "        'ctr': ctr\n",
    "    }\n",
    "\n",
    "# Global group —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - —Ç–æ–∂–µ —Å—Ä–∞–∑—É –≤ dict\n",
    "print(\"  Global group stats...\")\n",
    "global_group_dict = {}\n",
    "\n",
    "global_grouped = features_df.groupby('topic_code')['target']\n",
    "for post_group, targets in global_grouped:\n",
    "    views = len(targets)\n",
    "    likes = targets.sum()\n",
    "    ctr = likes / views if views > 0 else 0.05\n",
    "    \n",
    "    global_group_dict[post_group] = {\n",
    "        'ctr': ctr,\n",
    "        'ctr_tanh': np.tanh(ctr * 3) * 0.4,\n",
    "        'views': int(views),\n",
    "        'likes': int(likes)\n",
    "    }\n",
    "\n",
    "# User view counts (–¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞)\n",
    "print(\"  User view counts...\")\n",
    "user_view_counts_dict = features_df.groupby('user_id').size().to_dict()\n",
    "\n",
    "# Seen –ø–æ—Å—Ç—ã (–±—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø)\n",
    "print(\"  Seen posts...\")\n",
    "user_seen_dict = features_df.groupby('user_id')['post_id'].agg(set).to_dict()\n",
    "\n",
    "# Liked –ø–æ—Å—Ç—ã –∏–∑ df_likes (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ df_likes —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\n",
    "print(\"  Liked posts from df_likes...\")\n",
    "if 'df_likes' in locals() or 'df_likes' in globals():\n",
    "    user_liked_dict = df_likes.groupby('user_id')['post_id'].agg(set).to_dict()\n",
    "    print(f\"  Loaded {len(user_liked_dict):,} users from df_likes\")\n",
    "else:\n",
    "    # Fallback: —Å–æ–∑–¥–∞–µ–º –∏–∑ features_df –µ—Å–ª–∏ df_likes –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç\n",
    "    print(\"  WARNING: df_likes not found, using features_df fallback\")\n",
    "    liked_df = features_df[features_df['target'] == True]\n",
    "    user_liked_dict = liked_df.groupby('user_id')['post_id'].agg(set).to_dict()\n",
    "    del liked_df\n",
    "\n",
    "# –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "del features_df\n",
    "gc.collect()\n",
    "\n",
    "# ==================== 3. –°–û–ó–î–ê–ù–ò–ï FAISS –ò–ù–î–ï–ö–°–ê ====================\n",
    "print(\"\\n3. –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...\")\n",
    "dimension = post_factors.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è cosine similarity\n",
    "faiss_index.add(post_factors)\n",
    "print(f\"–ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {dimension}\")\n",
    "\n",
    "# ==================== 4. –£–ü–†–û–©–ï–ù–ù–ê–Ø –ò –ë–´–°–¢–†–ê–Ø –§–£–ù–ö–¶–ò–Ø –†–ê–°–ß–ï–¢–ê SCORE ====================\n",
    "\n",
    "def calculate_novelty_normalized(post_id, seen_set, min_post_id, max_post_id):\n",
    "    if post_id in seen_set:\n",
    "        return 0.8  # –ë—ã–ª–æ 0.3, —Å—Ç–∞–ª–æ 0.8 (–º–µ–Ω—å—à–µ —à—Ç—Ä–∞—Ñ)\n",
    "    \n",
    "    novelty_norm = (post_id - min_post_id) / (max_post_id - min_post_id + 1e-8)\n",
    "    return 1.0 + novelty_norm * 0.2  # –ë—ã–ª–æ 1.0, —Å—Ç–∞–ª–æ 0.2 (–º–µ–Ω—å—à–µ –±–æ–Ω—É—Å)\n",
    "\n",
    "\n",
    "def calculate_scores_with_novelty(cosines, post_ids, user_id, seen_set, liked_set):\n",
    "    \"\"\"–†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∞ —Å —É—á–µ—Ç–æ–º –Ω–æ–≤–∏–∑–Ω—ã —á–µ—Ä–µ–∑ post_id.\"\"\"\n",
    "    n = len(post_ids)\n",
    "    scores = cosines.copy()\n",
    "    \n",
    "    # –ù–∞—Ö–æ–¥–∏–º –º–∏–Ω/–º–∞–∫—Å ID –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "    min_id = min(post_ids)\n",
    "    max_id = max(post_ids)\n",
    "    \n",
    "    for i in range(n):\n",
    "        post_id = post_ids[i]\n",
    "        cosine = cosines[i]\n",
    "        \n",
    "        # 1. –ë–æ–Ω—É—Å/—à—Ç—Ä–∞—Ñ –∑–∞ –Ω–æ–≤–∏–∑–Ω—É (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä!)\n",
    "        novelty_bonus = calculate_novelty_normalized(\n",
    "            post_id, seen_set, min_id, max_id\n",
    "        )\n",
    "        scores[i] *= novelty_bonus\n",
    "        \n",
    "        # 2. Personal CTR (–≤—Ç–æ—Ä–∏—á–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä)\n",
    "        post_group = post_to_group_dict.get(post_id, 45)\n",
    "        key = (user_id, post_group)\n",
    "        \n",
    "        if key in user_group_dict:\n",
    "            stats = user_group_dict[key]\n",
    "            if stats['views'] >= 3 and stats['ctr'] > 0.1:\n",
    "                # –ê–¥–¥–∏—Ç–∏–≤–Ω—ã–π –±–æ–Ω—É—Å, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π\n",
    "                ctr_bonus = min(0.3, stats['ctr'] * 0.7)\n",
    "                scores[i] += ctr_bonus\n",
    "        \n",
    "        # 3. Global CTR (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ)\n",
    "        if post_group in global_group_dict:\n",
    "            g_bonus = global_group_dict[post_group]['ctr_tanh'] * 0.05\n",
    "            scores[i] += g_bonus\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# ==================== 5. –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ë–ê–¢–ß–ï–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê ====================\n",
    "def process_batch_fast(user_batch, k=300):\n",
    "    \"\"\"–°—É–ø–µ—Ä–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞.\"\"\"\n",
    "    batch_results = {}\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏\n",
    "    valid_users = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for uid in user_batch:\n",
    "        if uid in user_id_to_idx:\n",
    "            valid_users.append(uid)\n",
    "            valid_indices.append(user_id_to_idx[uid])\n",
    "    \n",
    "    if not valid_users:\n",
    "        return {uid: [] for uid in user_batch}\n",
    "    \n",
    "    # FAISS –ø–æ–∏—Å–∫ –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "    user_vectors = user_factors[valid_indices]\n",
    "    D_batch, I_batch = faiss_index.search(user_vectors, k=k)\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    for i, user_id in enumerate(valid_users):\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤\n",
    "        cosines = D_batch[i]\n",
    "        post_indices = I_batch[i]\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ post_id\n",
    "        candidate_post_ids = []\n",
    "        for idx in post_indices:\n",
    "            if idx < len(post_idx_to_id):\n",
    "                candidate_post_ids.append(post_idx_to_id[idx])\n",
    "        \n",
    "        if not candidate_post_ids:\n",
    "            batch_results[user_id] = []\n",
    "            continue\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º –ª–∞–π–∫–Ω—É—Ç—ã–µ (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)\n",
    "        liked_set = user_liked_dict.get(user_id, set())\n",
    "        seen_set = user_seen_dict.get(user_id, set())\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "        filtered_post_ids = []\n",
    "        filtered_cosines = []\n",
    "        \n",
    "        for post_id, cosine in zip(candidate_post_ids, cosines):\n",
    "            if post_id not in liked_set:\n",
    "                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "                if post_id in seen_set and cosine < 0.6:\n",
    "                    continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∞–±–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ\n",
    "                filtered_post_ids.append(post_id)\n",
    "                filtered_cosines.append(cosine)\n",
    "        \n",
    "        if not filtered_post_ids:\n",
    "            batch_results[user_id] = []\n",
    "            continue\n",
    "        \n",
    "        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∞\n",
    "        scores = calculate_scores_with_novelty(\n",
    "            np.array(filtered_cosines, dtype=np.float32),\n",
    "            filtered_post_ids,\n",
    "            user_id,\n",
    "            seen_set,\n",
    "            liked_set\n",
    "        )\n",
    "        \n",
    "        # –ë—ã—Å—Ç—Ä–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ç–æ–ø-N\n",
    "        top_n = 30\n",
    "        if len(scores) > top_n:\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º argpartition –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–æ–ø-N\n",
    "            top_indices = np.argpartition(-scores, top_n)[:top_n]\n",
    "            top_scores = scores[top_indices]\n",
    "            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N\n",
    "            sorted_top_indices = top_indices[np.argsort(-top_scores)]\n",
    "            top_post_ids = [filtered_post_ids[idx] for idx in sorted_top_indices]\n",
    "        else:\n",
    "            # –ï—Å–ª–∏ –º–∞–ª–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –ø—Ä–æ—Å—Ç–æ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ\n",
    "            sorted_indices = np.argsort(-scores)\n",
    "            top_post_ids = [filtered_post_ids[idx] for idx in sorted_indices]\n",
    "        \n",
    "        batch_results[user_id] = top_post_ids\n",
    "    \n",
    "    # –î–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –±–µ–∑ —Ñ–∞–∫—Ç–æ—Ä–æ–≤\n",
    "    for uid in user_batch:\n",
    "        if uid not in batch_results:\n",
    "            batch_results[uid] = []\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# ==================== 6. –ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò ====================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"–ó–ê–ü–£–°–ö –ë–ê–¢–ß–ï–í–û–ô –û–ë–†–ê–ë–û–¢–ö–ò\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# –í—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏\n",
    "all_user_ids = np.array(list(user_id_to_idx.keys()), dtype=np.int32)\n",
    "n_users = len(all_user_ids)\n",
    "print(f\"–í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {n_users:,}\")\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–∞—Ç—á–∏–Ω–≥–∞\n",
    "batch_size = 10000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –±–∞—Ç—á –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "all_results = {}\n",
    "\n",
    "# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π k –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è (6800 –ø–æ—Å—Ç–æ–≤)\n",
    "k_optimal = 300  # 4.4% –æ—Ç –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤ - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ!\n",
    "\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º k={k_optimal} (FAISS –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å {k_optimal} –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\")\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏\n",
    "n_batches = (n_users + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in tqdm(range(0, n_users, batch_size), total=n_batches, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π\"):\n",
    "    batch_end = min(batch_idx + batch_size, n_users)\n",
    "    user_batch = all_user_ids[batch_idx:batch_end]\n",
    "    \n",
    "    batch_results = process_batch_fast(user_batch, k=k_optimal)\n",
    "    all_results.update(batch_results)\n",
    "    \n",
    "    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –∫–∞–∂–¥—ã–µ 50K –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "    if batch_idx % 50000 == 0 and batch_idx > 0:\n",
    "        gc.collect()\n",
    "\n",
    "# ==================== 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================\n",
    "print(\"\\n7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ pickle\n",
    "with open('precomputed_candidates_fast_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è {len(all_results):,} —é–∑–µ—Ä–æ–≤\")\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "result_lengths = [len(posts) for posts in all_results.values()]\n",
    "counts_30 = sum(1 for x in result_lengths if x == 30)\n",
    "print(f\"–Æ–∑–µ—Ä–æ–≤ —Å 30 –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏: {counts_30:,} ({counts_30/n_users*100:.1f}%)\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ —é–∑–µ—Ä–∞\n",
    "if all_results:\n",
    "    first_user = list(all_results.keys())[0]\n",
    "    print(f\"\\n–ü—Ä–∏–º–µ—Ä –¥–ª—è —é–∑–µ—Ä–∞ {first_user}:\")\n",
    "    print(f\"  –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(all_results[first_user])}\")\n",
    "    if len(all_results[first_user]) > 0:\n",
    "        print(f\"  –ü–µ—Ä–≤—ã–µ 3: {all_results[first_user][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6acf335e-37f1-4a51-a542-3de331c77913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DataFrame\n",
    "sql_data = []\n",
    "for user_id, post_ids in all_results.items():\n",
    "    for position, post_id in enumerate(post_ids, 1):  # position –æ—Ç 1 –¥–æ N\n",
    "        sql_data.append({\n",
    "            'user_id': int(user_id),\n",
    "            'post_id': int(post_id),\n",
    "            'position': position,  # –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (1 - –ª—É—á—à–∏–π)\n",
    "        })\n",
    "\n",
    "df_sql_cand = pd.DataFrame(sql_data)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV –¥–ª—è COPY\n",
    "df_sql_cand.to_csv('recommendations_3.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e3651a-d6f5-4914-b273-e25033c9f709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>4049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>6582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>3503</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  post_id  position\n",
       "0      200     4049         1\n",
       "1      200     6582         2\n",
       "2      200     3503         3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql_cand.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73af309a-ad87-44e4-a516-a394a6cba7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4895040 entries, 0 to 4895039\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype\n",
      "---  ------    -----\n",
      " 0   user_id   int64\n",
      " 1   post_id   int64\n",
      " 2   position  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 112.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_sql_cand.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea394e59-212c-4024-85c3-ea5906f9281f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
